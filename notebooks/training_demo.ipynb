{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST GAN Training Demo\n",
    "\n",
    "This notebook demonstrates training a DCGAN on MNIST digits.\n",
    "\n",
    "The architecture features:\n",
    "- **Generator**: 100-dim latent -> 3x3x112 projection -> TransConv layers -> 28x28x1\n",
    "- **Discriminator**: 28x28x1 -> Conv layers -> single logit\n",
    "- **Training**: TTUR with 10:1 G/D learning rate ratio, exponential LR decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from src import GANTrainer, display_image_grid, print_device_info\n",
    "from src import NotebookImageCallback, ProgressCallback\n",
    "\n",
    "# Show device configuration\n",
    "print_device_info()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Trainer\n",
    "\n",
    "Create the trainer with MATLAB-matched hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer (values shown are defaults - adjust as needed)\n",
    "trainer = GANTrainer(\n",
    "    batch_size=120,\n",
    "    epochs=10,\n",
    "    g_learning_rate=0.0002,\n",
    "    d_learning_rate=0.0005,  # 10:1 ratio for stable training\n",
    "    lr_decay_rate=0.96,\n",
    "    lr_decay_steps=1000,\n",
    "    flip_factor=0.0,\n",
    "    latent_dim=100,\n",
    "    output_dir='../outputs/generated_samples'\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {trainer.batch_size}\")\n",
    "print(f\"Epochs: {trainer.epochs}\")\n",
    "print(f\"G learning rate: {trainer.g_learning_rate}\")\n",
    "print(f\"D learning rate: {trainer.d_learning_rate}\")\n",
    "print(f\"LR decay: {trainer.lr_decay_rate} every {trainer.lr_decay_steps} steps\")\n",
    "print(f\"Flip factor: {trainer.flip_factor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load and preprocess MNIST. Images are normalized to [-1, 1] to match tanh output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "dataset = trainer.load_data()\n",
    "\n",
    "# Show dataset info\n",
    "for batch in dataset.take(1):\n",
    "    print(f\"Batch shape: {batch.shape}\")\n",
    "    print(f\"Value range: [{batch.numpy().min():.2f}, {batch.numpy().max():.2f}]\")\n",
    "\n",
    "# Display sample real images\n",
    "sample_batch = next(iter(dataset))\n",
    "fig = display_image_grid(sample_batch[:25], title=\"Real MNIST Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model\n",
    "\n",
    "Create and compile the GAN with Generator and Discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = trainer.build_model()\n",
    "\n",
    "print(\"\\n=== Generator Architecture ===\")\n",
    "model.generator.summary()\n",
    "\n",
    "print(\"\\n=== Discriminator Architecture ===\")\n",
    "model.discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Forward Pass\n",
    "\n",
    "Verify the model works before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generator\n",
    "z = tf.random.normal((5, 100))\n",
    "fake_images = model.generator(z, training=False)\n",
    "print(f\"Generator output shape: {fake_images.shape}\")\n",
    "print(f\"Generator output range: [{fake_images.numpy().min():.3f}, {fake_images.numpy().max():.3f}]\")\n",
    "\n",
    "# Test discriminator\n",
    "real_batch = next(iter(dataset))[:5]\n",
    "real_logits = model.discriminator(real_batch, training=False)\n",
    "fake_logits = model.discriminator(fake_images, training=False)\n",
    "print(f\"\\nDiscriminator output shape: {real_logits.shape}\")\n",
    "print(f\"Real image logits: {real_logits.numpy().flatten()}\")\n",
    "print(f\"Fake image logits: {fake_logits.numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the GAN\n",
    "\n",
    "Train with progress visualization. Generated samples are saved to `outputs/generated_samples/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    NotebookImageCallback(num_samples=25, latent_dim=100),  # Updates image each epoch\n",
    "    ProgressCallback(print_frequency=100)  # Print progress every 100 batches\n",
    "]\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training for {trainer.epochs} epochs\")\n",
    "print(f\"Batches per epoch: {len(trainer.train_dataset)}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "history = trainer.train(callbacks=callbacks, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Results\n",
    "\n",
    "Visualize loss curves and generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history.history['g_loss'], label='Generator Loss')\n",
    "axes[0].plot(history.history['d_loss'], label='Discriminator Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Losses')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Score plot\n",
    "axes[1].plot(history.history['g_score'], label='Generator Score')\n",
    "axes[1].plot(history.history['d_score'], label='Discriminator Score')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Training Scores')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final samples\n",
    "final_samples = trainer.generate_samples(num_samples=25, seed=42)\n",
    "\n",
    "fig = display_image_grid(\n",
    "    final_samples,\n",
    "    title=f\"Generated Digits (After {trainer.epochs} Epochs)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Generation\n",
    "\n",
    "Generate new samples with different random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple batches with different seeds\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "for idx, (ax, seed) in enumerate(zip(axes.flatten(), [1, 42, 123, 999])):\n",
    "    samples = trainer.generate_samples(num_samples=16, seed=seed)\n",
    "    samples_np = (samples.numpy() + 1) / 2  # Rescale to [0, 1]\n",
    "    \n",
    "    # Create 4x4 grid\n",
    "    grid = samples_np.reshape(4, 4, 28, 28).transpose(0, 2, 1, 3).reshape(112, 112)\n",
    "    \n",
    "    ax.imshow(grid, cmap='gray')\n",
    "    ax.set_title(f'Seed: {seed}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Generated Samples with Different Seeds', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Latent Space Interpolation\n",
    "\n",
    "Interpolate between two latent vectors to see smooth transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src import sample_latent_vectors\n",
    "\n",
    "# Sample two random latent vectors\n",
    "z1 = sample_latent_vectors(1, seed=42)\n",
    "z2 = sample_latent_vectors(1, seed=123)\n",
    "\n",
    "# Interpolate\n",
    "num_steps = 10\n",
    "alphas = np.linspace(0, 1, num_steps)\n",
    "z_interp = tf.stack([z1[0] * (1 - a) + z2[0] * a for a in alphas])\n",
    "\n",
    "# Generate interpolated images\n",
    "interp_images = model.generate_from_latent(z_interp)\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, num_steps, figsize=(15, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = (interp_images[i].numpy() + 1) / 2\n",
    "    ax.imshow(img[:, :, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Î±={alphas[i]:.1f}')\n",
    "\n",
    "plt.suptitle('Latent Space Interpolation', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
